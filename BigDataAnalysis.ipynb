{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1 \u2013 Big Data Analysis with PySpark\n",
        "\n",
        "This notebook demonstrates big data analysis using **PySpark** on a synthetic retail transactions dataset.\n",
        "\n",
        "It covers:\n",
        "- Creating a Spark session\n",
        "- Generating a large synthetic dataset\n",
        "- Loading data into a Spark DataFrame\n",
        "- Running transformations and aggregations\n",
        "- Deriving business insights from the results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as spark_sum, count, avg, hour\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('CodTech_Big_Data_Analysis') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate synthetic large-scale retail transaction data\n",
        "num_records = 500000  # half a million rows as an example of big data\n",
        "\n",
        "customers = [f'CUST_{i:04d}' for i in range(1, 501)]\n",
        "products = [\n",
        "    ('PROD_A', 'Electronics'),\n",
        "    ('PROD_B', 'Groceries'),\n",
        "    ('PROD_C', 'Clothing'),\n",
        "    ('PROD_D', 'Home & Kitchen'),\n",
        "    ('PROD_E', 'Books')\n",
        "]\n",
        "\n",
        "start_date = datetime(2024, 1, 1)\n",
        "rows = []\n",
        "\n",
        "for i in range(num_records):\n",
        "    cust = random.choice(customers)\n",
        "    prod, category = random.choice(products)\n",
        "    quantity = random.randint(1, 5)\n",
        "    price = random.choice([199, 299, 399, 499, 799, 999, 1499])\n",
        "    amount = quantity * price\n",
        "    order_time = start_date + timedelta(minutes=random.randint(0, 60 * 24 * 90))\n",
        "\n",
        "    rows.append((i + 1, cust, prod, category, quantity, price, amount, order_time))\n",
        "\n",
        "columns = ['transaction_id', 'customer_id', 'product_id', 'category', 'quantity', 'price', 'amount', 'order_time']\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(rows)\n",
        "df = spark.createDataFrame(rdd, schema=columns)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Basic data exploration\n",
        "print('Total number of records:', df.count())\n",
        "\n",
        "df.describe(['quantity', 'price', 'amount']).show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Insight 1: Total revenue by product category\n",
        "revenue_by_category = df.groupBy('category').agg(\n",
        "    spark_sum('amount').alias('total_revenue'),\n",
        "    count('*').alias('num_transactions')\n",
        ").orderBy(col('total_revenue').desc())\n",
        "\n",
        "revenue_by_category.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Insight 2: Top 10 customers by total spending\n",
        "top_customers = df.groupBy('customer_id').agg(\n",
        "    spark_sum('amount').alias('total_spent'),\n",
        "    count('*').alias('num_orders')\n",
        ").orderBy(col('total_spent').desc()).limit(10)\n",
        "\n",
        "top_customers.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Insight 3: Revenue by hour of day (peak business hours)\n",
        "df_with_hour = df.withColumn('order_hour', hour(col('order_time')))\n",
        "\n",
        "revenue_by_hour = df_with_hour.groupBy('order_hour').agg(\n",
        "    spark_sum('amount').alias('total_revenue'),\n",
        "    count('*').alias('num_transactions')\n",
        ").orderBy('order_hour')\n",
        "\n",
        "revenue_by_hour.show(24)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Insights\n",
        "\n",
        "- **Top Revenue Categories:** Categories like Electronics or Home & Kitchen (depending on random run) generate the highest revenue.\n",
        "- **High-Value Customers:** We identified the top 10 customers by total spend, which can be targeted for loyalty programs.\n",
        "- **Peak Hours:** Revenue by hour of day shows when the store is busiest, helping in staffing and marketing decisions.\n",
        "\n",
        "This notebook demonstrates how PySpark can efficiently handle and analyze hundreds of thousands of records on a single machine while remaining scalable for larger clusters.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}